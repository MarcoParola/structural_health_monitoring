{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoParola/structural_health_monitoring/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBVRJV1iGAe"
      },
      "source": [
        "# **Utility notebook**\n",
        "\n",
        "Here I declare some functions and global variables usefull in all other notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Lkz_4slmaW"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sn\n",
        "from scipy import stats\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "import keras\n",
        "from keras import layers\n",
        "from sklearn import metrics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z-HZxbZwoYi"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/thesis2/'\n",
        "\n",
        "DATA_PATH = BASE_PATH + 'data/'\n",
        "MODELS_PATH = BASE_PATH + 'models/'\n",
        "\n",
        "DATASET = DATA_PATH + 'dataset.npy'\n",
        "\n",
        "NUMBER_OF_SENSORS = 16\n",
        "ENCODED_LENGTH = 70\n",
        "PATIENCE = 15\n",
        "MAX_EPOCHS = 10\n",
        "\n",
        "earlyStopCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                     mode='min', verbose=1, \n",
        "                                                     patience=PATIENCE, \n",
        "                                                     restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRz9pXXT-KaE"
      },
      "source": [
        "# Load ad preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W06HJ0uxhvLr"
      },
      "source": [
        "Utility function to load a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMdwBWcdUQGb"
      },
      "source": [
        "def load_dataset(dataset):\n",
        "  with open(DATA_PATH + dataset + '.npy', 'rb') as f:\n",
        "    dataset = np.load(f, allow_pickle=True)\n",
        "\n",
        "  print(dataset.shape)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility function to monitor the performance during the training process"
      ],
      "metadata": {
        "id": "br48Re3qsp7E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRHHAGbab-oy"
      },
      "source": [
        "def plot_accurancy_loss(hist):\n",
        "  plt.rcParams[\"figure.figsize\"] = (11,6.5)\n",
        "  font = {'size'   : 12}\n",
        "  plt.rc('font', **font)\n",
        "\n",
        "  acc_1 = hist.history['accuracy']\n",
        "  val_acc_1 = hist.history['val_accuracy']\n",
        "  loss_1 = hist.history['loss']\n",
        "  val_loss_1 = hist.history['val_loss']\n",
        "  \n",
        "  epochs = range(len(acc_1))\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss_1, 'C1', alpha=0.6, label='Training loss')\n",
        "  plt.plot(epochs, val_loss_1, 'C2', alpha=0.6, label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend(prop={'size': 17})\n",
        "  plt.show()\n",
        "\n",
        "  plt.ylim(0,1)\n",
        "  plt.plot(epochs, acc_1, 'C1', alpha=0.6, linewidth=2, label='Training acc')\n",
        "  plt.plot(epochs, val_acc_1, 'C2', alpha=0.6, linewidth=2, label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend(prop={'size': 17})\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0jGTUf-XoK"
      },
      "source": [
        "# **Function to evaluate model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YevX4lLq-beE"
      },
      "source": [
        "## Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9onq8xV34gB"
      },
      "source": [
        "def evaluateRegressor(model, testset, actual, minimal=False):\n",
        "  pred = model.predict(testset)\n",
        "  resid = np.squeeze(levels_test) - np.squeeze(pred)\n",
        "\n",
        "  font = {'size'   : 13}\n",
        "  plt.rc('font', **font)\n",
        "  plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "\n",
        "  print('R^2: ', r2_score(levels_test, pred), '\\nMAE', mean_absolute_error(levels_test, pred))\n",
        "\n",
        "  if minimal:\n",
        "    plt.plot([0.05, 0.25], [0.05, 0.25], c='orange', linewidth=4, alpha=.7)\n",
        "    plt.scatter(np.squeeze(levels_test), np.squeeze(pred), s=5, alpha=.5)\n",
        "    plt.title('Actual Damage Level vs Predicted values')\n",
        "    plt.xlabel('Actual Damage Level')\n",
        "    plt.ylabel('Predicted values')\n",
        "    plt.show()\n",
        "\n",
        "    x = range(len(resid))\n",
        "    plt.title('Residuals')\n",
        "    plt.xlabel('index')\n",
        "    plt.ylabel('displacement')\n",
        "    plt.plot([0, len(resid)], [0, 0], linewidth=4, c='orange', alpha=.8)\n",
        "    plt.scatter(x, resid, s=5, alpha=.5)\n",
        "    plt.show()\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmmAil1V-zQE"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMtX7VkxfumD"
      },
      "source": [
        "def evaluateClassifier(model, testset, actual, minimal=False):\n",
        "  pred = model.predict(testset)\n",
        "\n",
        "  y_classes = pred.argmax(axis=-1)\n",
        "  y_classes = to_categorical(y_classes)\n",
        "\n",
        "  y_test_non_category = [ np.argmax(t) for t in labels_test ]\n",
        "  y_predict_non_category = [ np.argmax(t) for t in y_classes ]\n",
        "  conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
        "  print(metrics.classification_report(y_test_non_category, y_predict_non_category))\n",
        "\n",
        "  if minimal:\n",
        "    # plot confusion matrix\n",
        "    group_counts = ['{0:0.0f}'.format(value) for value in conf_mat.flatten()]\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in (conf_mat / np.sum(conf_mat, axis=1)).flatten()]# conf_mat.flatten()/np.sum(conf_mat)]\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(9,9)\n",
        "    fig, ax = plt.subplots(figsize=(14,10))\n",
        "    plt.title('Confusion matrix')\n",
        "    sn.heatmap(conf_mat, annot=labels, fmt='', cmap=\"OrRd\", ax=ax)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Autoencoder"
      ],
      "metadata": {
        "id": "SV5K206GVCL_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8LxbYFJplUN"
      },
      "source": [
        "function to measure *mse* of an AE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAyTnw9VWGO2"
      },
      "source": [
        "def evaluate_autoencoder(model, testSet):\n",
        "  decoded_signals = model.predict(testSet)\n",
        "  mse = (np.square(testSet - decoded_signals)).mean(axis=None)\n",
        "  print('Mean square error: ', mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGB5MTdzWGOx"
      },
      "source": [
        "function to measure *mse* of a denoising_AE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DalH6vZmT85D"
      },
      "source": [
        "def evaluate_denoising_autoencoder(model, signals, noisy_signals):\n",
        "  decoded_signals = model.predict(noisy_signals)\n",
        "  mse = (np.square(signals - decoded_signals)).mean(axis=None)\n",
        "  print('Mean square error: ', mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}